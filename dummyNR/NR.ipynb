{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import random\n",
    "from math import exp\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/seeds_dataset.txt',delim_whitespace=True,header=None)\n",
    "features = list(data)[:-1]\n",
    "target = 7\n",
    "for feature in features:\n",
    "    curr_max = max(data[feature])\n",
    "    curr_min = min(data[feature])\n",
    "    data[feature] = data[feature].apply(lambda x : x/(curr_max-curr_min))\n",
    "data['output'] = data[target].apply(lambda x: [1 if i==x-1 else 0 for i in range(3)])\n",
    "data[target] = data[target].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         float64\n",
       "1         float64\n",
       "2         float64\n",
       "3         float64\n",
       "4         float64\n",
       "5         float64\n",
       "6         float64\n",
       "7           int64\n",
       "output     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def initialize_network(n_inputs, n_hidden,n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.4896935204622582, 0.029574963966907064, 0.04348729035652743]}]\n",
      "[{'weights': [0.703382088603836, 0.9831877173096739]}, {'weights': [0.5931837303800576, 0.393599686377914]}]\n"
     ]
    }
   ],
   "source": [
    "#test network\n",
    "network = initialize_network(2,1,2)\n",
    "for layer in network:\n",
    "    print layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Activation of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def activate(weights,inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i]*inputs[i]\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Transfer neuron activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transfer(activation):\n",
    "    return 1.0/(1.0 + exp(-activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def forward_propagate(network,row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'],inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7549481739306217, 0.7080550424834332]\n"
     ]
    }
   ],
   "source": [
    "# Testing forward propagation\n",
    "network = initialize_network(2,1,2)\n",
    "row = [1,0,None]     # Last one is the dummy input(bias 1)\n",
    "output = forward_propagate(network,row)\n",
    "print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transfer_derivative(output):\n",
    "    return output*(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def back_propagate_error(network,expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i+1]:\n",
    "                    error += (neuron['weights'][j]*neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j]-neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j]*transfer_derivative(neuron['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Testing backpropagation\n",
    "expected = [0,1]\n",
    "back_propagate_error(network,expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': 0.7599537265289951, 'weights': [0.17034919685568128, 0.5022385584334831, 0.9820766375385342], 'delta': -0.01016079516351957}]\n",
      "[{'output': 0.7549481739306217, 'weights': [0.7705231398308006, 0.5396174484497788], 'delta': -0.1396664907032541}, {'output': 0.7080550424834332, 'weights': [0.8602897789205496, 0.23217612806301458], 'delta': 0.06034884699244381}]\n"
     ]
    }
   ],
   "source": [
    "for layer in network:\n",
    "    print layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Updating network weights after getting errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def update_weights(network,row,l_rate): #l_rate:learining rate\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i-1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate*neuron['delta']*inputs[j]\n",
    "            neuron['weights'][-1] += l_rate*neuron['delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_network(network,train,l_rate,n_epoch,n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network,row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]-1] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            back_propagate_error(network,expected)\n",
    "            update_weights(network,row,l_rate)\n",
    "        print \"Iteration : \",epoch,\", Learning rate : \",l_rate,\", Sum of errors : \",sum_error\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  0 , Learning rate :  0.5 , Sum of errors :  6.36560716772\n",
      "Iteration :  1 , Learning rate :  0.5 , Sum of errors :  5.59766450753\n",
      "Iteration :  2 , Learning rate :  0.5 , Sum of errors :  5.37370225047\n",
      "Iteration :  3 , Learning rate :  0.5 , Sum of errors :  5.32681688461\n",
      "Iteration :  4 , Learning rate :  0.5 , Sum of errors :  5.26669974755\n",
      "Iteration :  5 , Learning rate :  0.5 , Sum of errors :  5.13742274206\n",
      "Iteration :  6 , Learning rate :  0.5 , Sum of errors :  4.96460675094\n",
      "Iteration :  7 , Learning rate :  0.5 , Sum of errors :  4.77357774934\n",
      "Iteration :  8 , Learning rate :  0.5 , Sum of errors :  4.54896132634\n",
      "Iteration :  9 , Learning rate :  0.5 , Sum of errors :  4.28425562944\n",
      "Iteration :  10 , Learning rate :  0.5 , Sum of errors :  3.98903023251\n",
      "Iteration :  11 , Learning rate :  0.5 , Sum of errors :  3.67880171937\n",
      "Iteration :  12 , Learning rate :  0.5 , Sum of errors :  3.36848702887\n",
      "Iteration :  13 , Learning rate :  0.5 , Sum of errors :  3.06977488333\n",
      "Iteration :  14 , Learning rate :  0.5 , Sum of errors :  2.79023063197\n",
      "Iteration :  15 , Learning rate :  0.5 , Sum of errors :  2.53368583284\n",
      "Iteration :  16 , Learning rate :  0.5 , Sum of errors :  2.30130559424\n",
      "Iteration :  17 , Learning rate :  0.5 , Sum of errors :  2.09263181028\n",
      "Iteration :  18 , Learning rate :  0.5 , Sum of errors :  1.90633191441\n",
      "Iteration :  19 , Learning rate :  0.5 , Sum of errors :  1.74065502157\n"
     ]
    }
   ],
   "source": [
    "seed(1)\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "n_inputs = len(dataset[0])-1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs,2,n_outputs)\n",
    "train_network(network,dataset,0.5,20,n_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': 0.9965606621574864, 'weights': [0.38645713364013506, 0.5764659087897724, 0.6916964513810357], 'delta': 0.0002994684914070449}, {'output': 0.03382652687004891, 'weights': [-1.3191491497141457, 1.6205210721287375, 0.811885442248185], 'delta': -0.0075667186028044895}]\n",
      "\n",
      "[{'output': 0.7065695133371781, 'weights': [0.8258105235746557, -1.900248018719592, 0.18075191605186908], 'delta': 0.06083665997965738}, {'output': 0.256453313500061, 'weights': [-0.8214066848336647, 2.3683320203807443, -0.3747776593797591], 'delta': -0.048901803032664434}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in network:\n",
    "    print layer\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict(network,row):\n",
    "    outputs = forward_propagate(network,row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected :  0 , Got :  1\n",
      "Expected :  0 , Got :  1\n",
      "Expected :  0 , Got :  1\n",
      "Expected :  0 , Got :  1\n",
      "Expected :  0 , Got :  1\n",
      "Expected :  1 , Got :  0\n",
      "Expected :  1 , Got :  0\n",
      "Expected :  1 , Got :  0\n",
      "Expected :  1 , Got :  0\n",
      "Expected :  1 , Got :  0\n"
     ]
    }
   ],
   "source": [
    "for row in dataset:\n",
    "    prediction = predict(network,row)\n",
    "    print \"Expected : \",row[-1], \", Got : \",prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Binding things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def back_propagation(train,l_rate,n_epoch,n_hidden):\n",
    "    n_inputs = len(train[0])-1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs,n_hidden,n_outputs)\n",
    "    train_network(network,train,l_rate,n_epoch,n_outputs)\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "for i in range(len(data)):\n",
    "    curr = list(data[features].iloc[i])\n",
    "    curr.append(int(data[target].iloc[i]))\n",
    "    train.append(list(curr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  0 , Learning rate :  0.3 , Sum of errors :  98.0153413192\n",
      "Iteration :  1 , Learning rate :  0.3 , Sum of errors :  70.6703123588\n",
      "Iteration :  2 , Learning rate :  0.3 , Sum of errors :  70.9005068008\n",
      "Iteration :  3 , Learning rate :  0.3 , Sum of errors :  70.9015366379\n",
      "Iteration :  4 , Learning rate :  0.3 , Sum of errors :  70.9013425253\n",
      "Iteration :  5 , Learning rate :  0.3 , Sum of errors :  70.9011483797\n",
      "Iteration :  6 , Learning rate :  0.3 , Sum of errors :  70.9009604549\n",
      "Iteration :  7 , Learning rate :  0.3 , Sum of errors :  70.9007784843\n",
      "Iteration :  8 , Learning rate :  0.3 , Sum of errors :  70.9006021852\n",
      "Iteration :  9 , Learning rate :  0.3 , Sum of errors :  70.9004312927\n",
      "Iteration :  10 , Learning rate :  0.3 , Sum of errors :  70.9002655581\n",
      "Iteration :  11 , Learning rate :  0.3 , Sum of errors :  70.9001047477\n",
      "Iteration :  12 , Learning rate :  0.3 , Sum of errors :  70.899948642\n",
      "Iteration :  13 , Learning rate :  0.3 , Sum of errors :  70.8997970343\n",
      "Iteration :  14 , Learning rate :  0.3 , Sum of errors :  70.8996497299\n",
      "Iteration :  15 , Learning rate :  0.3 , Sum of errors :  70.8995065455\n",
      "Iteration :  16 , Learning rate :  0.3 , Sum of errors :  70.8993673077\n",
      "Iteration :  17 , Learning rate :  0.3 , Sum of errors :  70.8992318531\n",
      "Iteration :  18 , Learning rate :  0.3 , Sum of errors :  70.8991000272\n",
      "Iteration :  19 , Learning rate :  0.3 , Sum of errors :  70.8989716837\n",
      "Iteration :  20 , Learning rate :  0.3 , Sum of errors :  70.8988466842\n",
      "Iteration :  21 , Learning rate :  0.3 , Sum of errors :  70.8987248977\n",
      "Iteration :  22 , Learning rate :  0.3 , Sum of errors :  70.8986061999\n",
      "Iteration :  23 , Learning rate :  0.3 , Sum of errors :  70.8984904729\n",
      "Iteration :  24 , Learning rate :  0.3 , Sum of errors :  70.8983776047\n",
      "Iteration :  25 , Learning rate :  0.3 , Sum of errors :  70.898267489\n",
      "Iteration :  26 , Learning rate :  0.3 , Sum of errors :  70.8981600247\n",
      "Iteration :  27 , Learning rate :  0.3 , Sum of errors :  70.8980551156\n",
      "Iteration :  28 , Learning rate :  0.3 , Sum of errors :  70.8979526703\n",
      "Iteration :  29 , Learning rate :  0.3 , Sum of errors :  70.8978526016\n",
      "Iteration :  30 , Learning rate :  0.3 , Sum of errors :  70.8977548263\n",
      "Iteration :  31 , Learning rate :  0.3 , Sum of errors :  70.8976592653\n",
      "Iteration :  32 , Learning rate :  0.3 , Sum of errors :  70.897565843\n",
      "Iteration :  33 , Learning rate :  0.3 , Sum of errors :  70.8974744872\n",
      "Iteration :  34 , Learning rate :  0.3 , Sum of errors :  70.8973851291\n",
      "Iteration :  35 , Learning rate :  0.3 , Sum of errors :  70.8972977028\n",
      "Iteration :  36 , Learning rate :  0.3 , Sum of errors :  70.8972121454\n",
      "Iteration :  37 , Learning rate :  0.3 , Sum of errors :  70.8971283966\n",
      "Iteration :  38 , Learning rate :  0.3 , Sum of errors :  70.8970463987\n",
      "Iteration :  39 , Learning rate :  0.3 , Sum of errors :  70.8969660967\n",
      "Iteration :  40 , Learning rate :  0.3 , Sum of errors :  70.8968874376\n",
      "Iteration :  41 , Learning rate :  0.3 , Sum of errors :  70.8968103707\n",
      "Iteration :  42 , Learning rate :  0.3 , Sum of errors :  70.8967348475\n",
      "Iteration :  43 , Learning rate :  0.3 , Sum of errors :  70.8966608213\n",
      "Iteration :  44 , Learning rate :  0.3 , Sum of errors :  70.8965882474\n",
      "Iteration :  45 , Learning rate :  0.3 , Sum of errors :  70.8965170829\n",
      "Iteration :  46 , Learning rate :  0.3 , Sum of errors :  70.8964472865\n",
      "Iteration :  47 , Learning rate :  0.3 , Sum of errors :  70.8963788185\n",
      "Iteration :  48 , Learning rate :  0.3 , Sum of errors :  70.896311641\n",
      "Iteration :  49 , Learning rate :  0.3 , Sum of errors :  70.8962457171\n",
      "Iteration :  50 , Learning rate :  0.3 , Sum of errors :  70.8961810117\n",
      "Iteration :  51 , Learning rate :  0.3 , Sum of errors :  70.8961174908\n",
      "Iteration :  52 , Learning rate :  0.3 , Sum of errors :  70.8960551218\n",
      "Iteration :  53 , Learning rate :  0.3 , Sum of errors :  70.8959938732\n",
      "Iteration :  54 , Learning rate :  0.3 , Sum of errors :  70.8959337147\n",
      "Iteration :  55 , Learning rate :  0.3 , Sum of errors :  70.895874617\n",
      "Iteration :  56 , Learning rate :  0.3 , Sum of errors :  70.8958165521\n",
      "Iteration :  57 , Learning rate :  0.3 , Sum of errors :  70.8957594928\n",
      "Iteration :  58 , Learning rate :  0.3 , Sum of errors :  70.8957034128\n",
      "Iteration :  59 , Learning rate :  0.3 , Sum of errors :  70.8956482868\n",
      "Iteration :  60 , Learning rate :  0.3 , Sum of errors :  70.8955940905\n",
      "Iteration :  61 , Learning rate :  0.3 , Sum of errors :  70.8955408003\n",
      "Iteration :  62 , Learning rate :  0.3 , Sum of errors :  70.8954883934\n",
      "Iteration :  63 , Learning rate :  0.3 , Sum of errors :  70.8954368478\n",
      "Iteration :  64 , Learning rate :  0.3 , Sum of errors :  70.8953861422\n",
      "Iteration :  65 , Learning rate :  0.3 , Sum of errors :  70.895336256\n",
      "Iteration :  66 , Learning rate :  0.3 , Sum of errors :  70.8952871694\n",
      "Iteration :  67 , Learning rate :  0.3 , Sum of errors :  70.8952388631\n",
      "Iteration :  68 , Learning rate :  0.3 , Sum of errors :  70.8951913186\n",
      "Iteration :  69 , Learning rate :  0.3 , Sum of errors :  70.8951445177\n",
      "Iteration :  70 , Learning rate :  0.3 , Sum of errors :  70.8950984431\n",
      "Iteration :  71 , Learning rate :  0.3 , Sum of errors :  70.8950530778\n",
      "Iteration :  72 , Learning rate :  0.3 , Sum of errors :  70.8950084055\n",
      "Iteration :  73 , Learning rate :  0.3 , Sum of errors :  70.8949644103\n",
      "Iteration :  74 , Learning rate :  0.3 , Sum of errors :  70.8949210768\n",
      "Iteration :  75 , Learning rate :  0.3 , Sum of errors :  70.8948783902\n",
      "Iteration :  76 , Learning rate :  0.3 , Sum of errors :  70.894836336\n",
      "Iteration :  77 , Learning rate :  0.3 , Sum of errors :  70.8947949002\n",
      "Iteration :  78 , Learning rate :  0.3 , Sum of errors :  70.8947540691\n",
      "Iteration :  79 , Learning rate :  0.3 , Sum of errors :  70.8947138297\n",
      "Iteration :  80 , Learning rate :  0.3 , Sum of errors :  70.894674169\n",
      "Iteration :  81 , Learning rate :  0.3 , Sum of errors :  70.8946350747\n",
      "Iteration :  82 , Learning rate :  0.3 , Sum of errors :  70.8945965347\n",
      "Iteration :  83 , Learning rate :  0.3 , Sum of errors :  70.8945585373\n",
      "Iteration :  84 , Learning rate :  0.3 , Sum of errors :  70.8945210711\n",
      "Iteration :  85 , Learning rate :  0.3 , Sum of errors :  70.894484125\n",
      "Iteration :  86 , Learning rate :  0.3 , Sum of errors :  70.8944476882\n",
      "Iteration :  87 , Learning rate :  0.3 , Sum of errors :  70.8944117504\n",
      "Iteration :  88 , Learning rate :  0.3 , Sum of errors :  70.8943763013\n",
      "Iteration :  89 , Learning rate :  0.3 , Sum of errors :  70.8943413312\n",
      "Iteration :  90 , Learning rate :  0.3 , Sum of errors :  70.8943068302\n",
      "Iteration :  91 , Learning rate :  0.3 , Sum of errors :  70.8942727892\n",
      "Iteration :  92 , Learning rate :  0.3 , Sum of errors :  70.894239199\n",
      "Iteration :  93 , Learning rate :  0.3 , Sum of errors :  70.8942060508\n",
      "Iteration :  94 , Learning rate :  0.3 , Sum of errors :  70.894173336\n",
      "Iteration :  95 , Learning rate :  0.3 , Sum of errors :  70.894141046\n",
      "Iteration :  96 , Learning rate :  0.3 , Sum of errors :  70.8941091729\n",
      "Iteration :  97 , Learning rate :  0.3 , Sum of errors :  70.8940777087\n",
      "Iteration :  98 , Learning rate :  0.3 , Sum of errors :  70.8940466455\n",
      "Iteration :  99 , Learning rate :  0.3 , Sum of errors :  70.8940159758\n",
      "Iteration :  100 , Learning rate :  0.3 , Sum of errors :  70.8939856923\n",
      "Iteration :  101 , Learning rate :  0.3 , Sum of errors :  70.8939557879\n",
      "Iteration :  102 , Learning rate :  0.3 , Sum of errors :  70.8939262554\n",
      "Iteration :  103 , Learning rate :  0.3 , Sum of errors :  70.8938970882\n",
      "Iteration :  104 , Learning rate :  0.3 , Sum of errors :  70.8938682795\n",
      "Iteration :  105 , Learning rate :  0.3 , Sum of errors :  70.893839823\n",
      "Iteration :  106 , Learning rate :  0.3 , Sum of errors :  70.8938117122\n",
      "Iteration :  107 , Learning rate :  0.3 , Sum of errors :  70.893783941\n",
      "Iteration :  108 , Learning rate :  0.3 , Sum of errors :  70.8937565034\n",
      "Iteration :  109 , Learning rate :  0.3 , Sum of errors :  70.8937293935\n",
      "Iteration :  110 , Learning rate :  0.3 , Sum of errors :  70.8937026056\n",
      "Iteration :  111 , Learning rate :  0.3 , Sum of errors :  70.8936761341\n",
      "Iteration :  112 , Learning rate :  0.3 , Sum of errors :  70.8936499736\n",
      "Iteration :  113 , Learning rate :  0.3 , Sum of errors :  70.8936241187\n",
      "Iteration :  114 , Learning rate :  0.3 , Sum of errors :  70.8935985642\n",
      "Iteration :  115 , Learning rate :  0.3 , Sum of errors :  70.893573305\n",
      "Iteration :  116 , Learning rate :  0.3 , Sum of errors :  70.8935483362\n",
      "Iteration :  117 , Learning rate :  0.3 , Sum of errors :  70.8935236529\n",
      "Iteration :  118 , Learning rate :  0.3 , Sum of errors :  70.8934992503\n",
      "Iteration :  119 , Learning rate :  0.3 , Sum of errors :  70.893475124\n",
      "Iteration :  120 , Learning rate :  0.3 , Sum of errors :  70.8934512692\n",
      "Iteration :  121 , Learning rate :  0.3 , Sum of errors :  70.8934276816\n",
      "Iteration :  122 , Learning rate :  0.3 , Sum of errors :  70.8934043569\n",
      "Iteration :  123 , Learning rate :  0.3 , Sum of errors :  70.8933812908\n",
      "Iteration :  124 , Learning rate :  0.3 , Sum of errors :  70.8933584792\n",
      "Iteration :  125 , Learning rate :  0.3 , Sum of errors :  70.8933359181\n",
      "Iteration :  126 , Learning rate :  0.3 , Sum of errors :  70.8933136035\n",
      "Iteration :  127 , Learning rate :  0.3 , Sum of errors :  70.8932915315\n",
      "Iteration :  128 , Learning rate :  0.3 , Sum of errors :  70.8932696983\n",
      "Iteration :  129 , Learning rate :  0.3 , Sum of errors :  70.8932481003\n",
      "Iteration :  130 , Learning rate :  0.3 , Sum of errors :  70.8932267337\n",
      "Iteration :  131 , Learning rate :  0.3 , Sum of errors :  70.8932055952\n",
      "Iteration :  132 , Learning rate :  0.3 , Sum of errors :  70.8931846811\n",
      "Iteration :  133 , Learning rate :  0.3 , Sum of errors :  70.8931639881\n",
      "Iteration :  134 , Learning rate :  0.3 , Sum of errors :  70.8931435129\n",
      "Iteration :  135 , Learning rate :  0.3 , Sum of errors :  70.8931232522\n",
      "Iteration :  136 , Learning rate :  0.3 , Sum of errors :  70.8931032028\n",
      "Iteration :  137 , Learning rate :  0.3 , Sum of errors :  70.8930833616\n",
      "Iteration :  138 , Learning rate :  0.3 , Sum of errors :  70.8930637255\n",
      "Iteration :  139 , Learning rate :  0.3 , Sum of errors :  70.8930442915\n",
      "Iteration :  140 , Learning rate :  0.3 , Sum of errors :  70.8930250567\n",
      "Iteration :  141 , Learning rate :  0.3 , Sum of errors :  70.8930060182\n",
      "Iteration :  142 , Learning rate :  0.3 , Sum of errors :  70.8929871732\n",
      "Iteration :  143 , Learning rate :  0.3 , Sum of errors :  70.892968519\n",
      "Iteration :  144 , Learning rate :  0.3 , Sum of errors :  70.8929500527\n",
      "Iteration :  145 , Learning rate :  0.3 , Sum of errors :  70.8929317718\n",
      "Iteration :  146 , Learning rate :  0.3 , Sum of errors :  70.8929136737\n",
      "Iteration :  147 , Learning rate :  0.3 , Sum of errors :  70.8928957558\n",
      "Iteration :  148 , Learning rate :  0.3 , Sum of errors :  70.8928780155\n",
      "Iteration :  149 , Learning rate :  0.3 , Sum of errors :  70.8928604505\n",
      "Iteration :  150 , Learning rate :  0.3 , Sum of errors :  70.8928430584\n",
      "Iteration :  151 , Learning rate :  0.3 , Sum of errors :  70.8928258367\n",
      "Iteration :  152 , Learning rate :  0.3 , Sum of errors :  70.8928087833\n",
      "Iteration :  153 , Learning rate :  0.3 , Sum of errors :  70.8927918957\n",
      "Iteration :  154 , Learning rate :  0.3 , Sum of errors :  70.8927751718\n",
      "Iteration :  155 , Learning rate :  0.3 , Sum of errors :  70.8927586093\n",
      "Iteration :  156 , Learning rate :  0.3 , Sum of errors :  70.8927422063\n",
      "Iteration :  157 , Learning rate :  0.3 , Sum of errors :  70.8927259604\n",
      "Iteration :  158 , Learning rate :  0.3 , Sum of errors :  70.8927098698\n",
      "Iteration :  159 , Learning rate :  0.3 , Sum of errors :  70.8926939322\n",
      "Iteration :  160 , Learning rate :  0.3 , Sum of errors :  70.8926781459\n",
      "Iteration :  161 , Learning rate :  0.3 , Sum of errors :  70.8926625087\n",
      "Iteration :  162 , Learning rate :  0.3 , Sum of errors :  70.8926470189\n",
      "Iteration :  163 , Learning rate :  0.3 , Sum of errors :  70.8926316745\n",
      "Iteration :  164 , Learning rate :  0.3 , Sum of errors :  70.8926164736\n",
      "Iteration :  165 , Learning rate :  0.3 , Sum of errors :  70.8926014145\n",
      "Iteration :  166 , Learning rate :  0.3 , Sum of errors :  70.8925864953\n",
      "Iteration :  167 , Learning rate :  0.3 , Sum of errors :  70.8925717144\n",
      "Iteration :  168 , Learning rate :  0.3 , Sum of errors :  70.8925570699\n",
      "Iteration :  169 , Learning rate :  0.3 , Sum of errors :  70.8925425603\n",
      "Iteration :  170 , Learning rate :  0.3 , Sum of errors :  70.8925281838\n",
      "Iteration :  171 , Learning rate :  0.3 , Sum of errors :  70.8925139389\n",
      "Iteration :  172 , Learning rate :  0.3 , Sum of errors :  70.8924998239\n",
      "Iteration :  173 , Learning rate :  0.3 , Sum of errors :  70.8924858372\n",
      "Iteration :  174 , Learning rate :  0.3 , Sum of errors :  70.8924719774\n",
      "Iteration :  175 , Learning rate :  0.3 , Sum of errors :  70.8924582428\n",
      "Iteration :  176 , Learning rate :  0.3 , Sum of errors :  70.8924446321\n",
      "Iteration :  177 , Learning rate :  0.3 , Sum of errors :  70.8924311437\n",
      "Iteration :  178 , Learning rate :  0.3 , Sum of errors :  70.8924177761\n",
      "Iteration :  179 , Learning rate :  0.3 , Sum of errors :  70.8924045281\n",
      "Iteration :  180 , Learning rate :  0.3 , Sum of errors :  70.8923913981\n",
      "Iteration :  181 , Learning rate :  0.3 , Sum of errors :  70.8923783849\n",
      "Iteration :  182 , Learning rate :  0.3 , Sum of errors :  70.892365487\n",
      "Iteration :  183 , Learning rate :  0.3 , Sum of errors :  70.8923527032\n",
      "Iteration :  184 , Learning rate :  0.3 , Sum of errors :  70.8923400321\n",
      "Iteration :  185 , Learning rate :  0.3 , Sum of errors :  70.8923274725\n",
      "Iteration :  186 , Learning rate :  0.3 , Sum of errors :  70.8923150231\n",
      "Iteration :  187 , Learning rate :  0.3 , Sum of errors :  70.8923026826\n",
      "Iteration :  188 , Learning rate :  0.3 , Sum of errors :  70.89229045\n",
      "Iteration :  189 , Learning rate :  0.3 , Sum of errors :  70.8922783238\n",
      "Iteration :  190 , Learning rate :  0.3 , Sum of errors :  70.8922663031\n",
      "Iteration :  191 , Learning rate :  0.3 , Sum of errors :  70.8922543866\n",
      "Iteration :  192 , Learning rate :  0.3 , Sum of errors :  70.8922425731\n",
      "Iteration :  193 , Learning rate :  0.3 , Sum of errors :  70.8922308617\n",
      "Iteration :  194 , Learning rate :  0.3 , Sum of errors :  70.892219251\n",
      "Iteration :  195 , Learning rate :  0.3 , Sum of errors :  70.8922077402\n",
      "Iteration :  196 , Learning rate :  0.3 , Sum of errors :  70.8921963281\n",
      "Iteration :  197 , Learning rate :  0.3 , Sum of errors :  70.8921850136\n",
      "Iteration :  198 , Learning rate :  0.3 , Sum of errors :  70.8921737957\n",
      "Iteration :  199 , Learning rate :  0.3 , Sum of errors :  70.8921626735\n",
      "Iteration :  200 , Learning rate :  0.3 , Sum of errors :  70.8921516458\n",
      "Iteration :  201 , Learning rate :  0.3 , Sum of errors :  70.8921407118\n",
      "Iteration :  202 , Learning rate :  0.3 , Sum of errors :  70.8921298704\n",
      "Iteration :  203 , Learning rate :  0.3 , Sum of errors :  70.8921191207\n",
      "Iteration :  204 , Learning rate :  0.3 , Sum of errors :  70.8921084617\n",
      "Iteration :  205 , Learning rate :  0.3 , Sum of errors :  70.8920978926\n",
      "Iteration :  206 , Learning rate :  0.3 , Sum of errors :  70.8920874123\n",
      "Iteration :  207 , Learning rate :  0.3 , Sum of errors :  70.8920770201\n",
      "Iteration :  208 , Learning rate :  0.3 , Sum of errors :  70.8920667151\n",
      "Iteration :  209 , Learning rate :  0.3 , Sum of errors :  70.8920564963\n",
      "Iteration :  210 , Learning rate :  0.3 , Sum of errors :  70.8920463628\n",
      "Iteration :  211 , Learning rate :  0.3 , Sum of errors :  70.892036314\n",
      "Iteration :  212 , Learning rate :  0.3 , Sum of errors :  70.8920263489\n",
      "Iteration :  213 , Learning rate :  0.3 , Sum of errors :  70.8920164667\n",
      "Iteration :  214 , Learning rate :  0.3 , Sum of errors :  70.8920066666\n",
      "Iteration :  215 , Learning rate :  0.3 , Sum of errors :  70.8919969478\n",
      "Iteration :  216 , Learning rate :  0.3 , Sum of errors :  70.8919873096\n",
      "Iteration :  217 , Learning rate :  0.3 , Sum of errors :  70.8919777511\n",
      "Iteration :  218 , Learning rate :  0.3 , Sum of errors :  70.8919682716\n",
      "Iteration :  219 , Learning rate :  0.3 , Sum of errors :  70.8919588704\n",
      "Iteration :  220 , Learning rate :  0.3 , Sum of errors :  70.8919495467\n",
      "Iteration :  221 , Learning rate :  0.3 , Sum of errors :  70.8919402998\n",
      "Iteration :  222 , Learning rate :  0.3 , Sum of errors :  70.891931129\n",
      "Iteration :  223 , Learning rate :  0.3 , Sum of errors :  70.8919220335\n",
      "Iteration :  224 , Learning rate :  0.3 , Sum of errors :  70.8919130128\n",
      "Iteration :  225 , Learning rate :  0.3 , Sum of errors :  70.891904066\n",
      "Iteration :  226 , Learning rate :  0.3 , Sum of errors :  70.8918951926\n",
      "Iteration :  227 , Learning rate :  0.3 , Sum of errors :  70.8918863918\n",
      "Iteration :  228 , Learning rate :  0.3 , Sum of errors :  70.8918776631\n",
      "Iteration :  229 , Learning rate :  0.3 , Sum of errors :  70.8918690057\n",
      "Iteration :  230 , Learning rate :  0.3 , Sum of errors :  70.891860419\n",
      "Iteration :  231 , Learning rate :  0.3 , Sum of errors :  70.8918519025\n",
      "Iteration :  232 , Learning rate :  0.3 , Sum of errors :  70.8918434554\n",
      "Iteration :  233 , Learning rate :  0.3 , Sum of errors :  70.8918350772\n",
      "Iteration :  234 , Learning rate :  0.3 , Sum of errors :  70.8918267673\n",
      "Iteration :  235 , Learning rate :  0.3 , Sum of errors :  70.8918185251\n",
      "Iteration :  236 , Learning rate :  0.3 , Sum of errors :  70.89181035\n",
      "Iteration :  237 , Learning rate :  0.3 , Sum of errors :  70.8918022415\n",
      "Iteration :  238 , Learning rate :  0.3 , Sum of errors :  70.8917941989\n",
      "Iteration :  239 , Learning rate :  0.3 , Sum of errors :  70.8917862217\n",
      "Iteration :  240 , Learning rate :  0.3 , Sum of errors :  70.8917783094\n",
      "Iteration :  241 , Learning rate :  0.3 , Sum of errors :  70.8917704614\n",
      "Iteration :  242 , Learning rate :  0.3 , Sum of errors :  70.8917626772\n",
      "Iteration :  243 , Learning rate :  0.3 , Sum of errors :  70.8917549563\n",
      "Iteration :  244 , Learning rate :  0.3 , Sum of errors :  70.8917472981\n",
      "Iteration :  245 , Learning rate :  0.3 , Sum of errors :  70.8917397021\n",
      "Iteration :  246 , Learning rate :  0.3 , Sum of errors :  70.8917321679\n",
      "Iteration :  247 , Learning rate :  0.3 , Sum of errors :  70.8917246949\n",
      "Iteration :  248 , Learning rate :  0.3 , Sum of errors :  70.8917172826\n",
      "Iteration :  249 , Learning rate :  0.3 , Sum of errors :  70.8917099306\n",
      "Iteration :  250 , Learning rate :  0.3 , Sum of errors :  70.8917026384\n",
      "Iteration :  251 , Learning rate :  0.3 , Sum of errors :  70.8916954055\n",
      "Iteration :  252 , Learning rate :  0.3 , Sum of errors :  70.8916882314\n",
      "Iteration :  253 , Learning rate :  0.3 , Sum of errors :  70.8916811157\n",
      "Iteration :  254 , Learning rate :  0.3 , Sum of errors :  70.891674058\n",
      "Iteration :  255 , Learning rate :  0.3 , Sum of errors :  70.8916670577\n",
      "Iteration :  256 , Learning rate :  0.3 , Sum of errors :  70.8916601145\n",
      "Iteration :  257 , Learning rate :  0.3 , Sum of errors :  70.8916532279\n",
      "Iteration :  258 , Learning rate :  0.3 , Sum of errors :  70.8916463976\n",
      "Iteration :  259 , Learning rate :  0.3 , Sum of errors :  70.891639623\n",
      "Iteration :  260 , Learning rate :  0.3 , Sum of errors :  70.8916329037\n",
      "Iteration :  261 , Learning rate :  0.3 , Sum of errors :  70.8916262395\n",
      "Iteration :  262 , Learning rate :  0.3 , Sum of errors :  70.8916196297\n",
      "Iteration :  263 , Learning rate :  0.3 , Sum of errors :  70.8916130742\n",
      "Iteration :  264 , Learning rate :  0.3 , Sum of errors :  70.8916065723\n",
      "Iteration :  265 , Learning rate :  0.3 , Sum of errors :  70.8916001239\n",
      "Iteration :  266 , Learning rate :  0.3 , Sum of errors :  70.8915937285\n",
      "Iteration :  267 , Learning rate :  0.3 , Sum of errors :  70.8915873856\n",
      "Iteration :  268 , Learning rate :  0.3 , Sum of errors :  70.8915810951\n",
      "Iteration :  269 , Learning rate :  0.3 , Sum of errors :  70.8915748564\n",
      "Iteration :  270 , Learning rate :  0.3 , Sum of errors :  70.8915686692\n",
      "Iteration :  271 , Learning rate :  0.3 , Sum of errors :  70.8915625332\n",
      "Iteration :  272 , Learning rate :  0.3 , Sum of errors :  70.891556448\n",
      "Iteration :  273 , Learning rate :  0.3 , Sum of errors :  70.8915504133\n",
      "Iteration :  274 , Learning rate :  0.3 , Sum of errors :  70.8915444287\n",
      "Iteration :  275 , Learning rate :  0.3 , Sum of errors :  70.8915384939\n",
      "Iteration :  276 , Learning rate :  0.3 , Sum of errors :  70.8915326085\n",
      "Iteration :  277 , Learning rate :  0.3 , Sum of errors :  70.8915267723\n",
      "Iteration :  278 , Learning rate :  0.3 , Sum of errors :  70.8915209849\n",
      "Iteration :  279 , Learning rate :  0.3 , Sum of errors :  70.891515246\n",
      "Iteration :  280 , Learning rate :  0.3 , Sum of errors :  70.8915095553\n",
      "Iteration :  281 , Learning rate :  0.3 , Sum of errors :  70.8915039125\n",
      "Iteration :  282 , Learning rate :  0.3 , Sum of errors :  70.8914983172\n",
      "Iteration :  283 , Learning rate :  0.3 , Sum of errors :  70.8914927691\n",
      "Iteration :  284 , Learning rate :  0.3 , Sum of errors :  70.8914872681\n",
      "Iteration :  285 , Learning rate :  0.3 , Sum of errors :  70.8914818137\n",
      "Iteration :  286 , Learning rate :  0.3 , Sum of errors :  70.8914764057\n",
      "Iteration :  287 , Learning rate :  0.3 , Sum of errors :  70.8914710438\n",
      "Iteration :  288 , Learning rate :  0.3 , Sum of errors :  70.8914657278\n",
      "Iteration :  289 , Learning rate :  0.3 , Sum of errors :  70.8914604573\n",
      "Iteration :  290 , Learning rate :  0.3 , Sum of errors :  70.891455232\n",
      "Iteration :  291 , Learning rate :  0.3 , Sum of errors :  70.8914500518\n",
      "Iteration :  292 , Learning rate :  0.3 , Sum of errors :  70.8914449163\n",
      "Iteration :  293 , Learning rate :  0.3 , Sum of errors :  70.8914398253\n",
      "Iteration :  294 , Learning rate :  0.3 , Sum of errors :  70.8914347785\n",
      "Iteration :  295 , Learning rate :  0.3 , Sum of errors :  70.8914297756\n",
      "Iteration :  296 , Learning rate :  0.3 , Sum of errors :  70.8914248165\n",
      "Iteration :  297 , Learning rate :  0.3 , Sum of errors :  70.8914199009\n",
      "Iteration :  298 , Learning rate :  0.3 , Sum of errors :  70.8914150285\n",
      "Iteration :  299 , Learning rate :  0.3 , Sum of errors :  70.8914101991\n",
      "Iteration :  300 , Learning rate :  0.3 , Sum of errors :  70.8914054124\n",
      "Iteration :  301 , Learning rate :  0.3 , Sum of errors :  70.8914006682\n",
      "Iteration :  302 , Learning rate :  0.3 , Sum of errors :  70.8913959664\n",
      "Iteration :  303 , Learning rate :  0.3 , Sum of errors :  70.8913913066\n",
      "Iteration :  304 , Learning rate :  0.3 , Sum of errors :  70.8913866887\n",
      "Iteration :  305 , Learning rate :  0.3 , Sum of errors :  70.8913821123\n",
      "Iteration :  306 , Learning rate :  0.3 , Sum of errors :  70.8913775774\n",
      "Iteration :  307 , Learning rate :  0.3 , Sum of errors :  70.8913730837\n",
      "Iteration :  308 , Learning rate :  0.3 , Sum of errors :  70.891368631\n",
      "Iteration :  309 , Learning rate :  0.3 , Sum of errors :  70.891364219\n",
      "Iteration :  310 , Learning rate :  0.3 , Sum of errors :  70.8913598476\n",
      "Iteration :  311 , Learning rate :  0.3 , Sum of errors :  70.8913555166\n",
      "Iteration :  312 , Learning rate :  0.3 , Sum of errors :  70.8913512258\n",
      "Iteration :  313 , Learning rate :  0.3 , Sum of errors :  70.8913469749\n",
      "Iteration :  314 , Learning rate :  0.3 , Sum of errors :  70.8913427639\n",
      "Iteration :  315 , Learning rate :  0.3 , Sum of errors :  70.8913385924\n",
      "Iteration :  316 , Learning rate :  0.3 , Sum of errors :  70.8913344604\n",
      "Iteration :  317 , Learning rate :  0.3 , Sum of errors :  70.8913303676\n",
      "Iteration :  318 , Learning rate :  0.3 , Sum of errors :  70.8913263138\n",
      "Iteration :  319 , Learning rate :  0.3 , Sum of errors :  70.8913222989\n",
      "Iteration :  320 , Learning rate :  0.3 , Sum of errors :  70.8913183228\n",
      "Iteration :  321 , Learning rate :  0.3 , Sum of errors :  70.8913143851\n",
      "Iteration :  322 , Learning rate :  0.3 , Sum of errors :  70.8913104858\n",
      "Iteration :  323 , Learning rate :  0.3 , Sum of errors :  70.8913066248\n",
      "Iteration :  324 , Learning rate :  0.3 , Sum of errors :  70.8913028017\n",
      "Iteration :  325 , Learning rate :  0.3 , Sum of errors :  70.8912990165\n",
      "Iteration :  326 , Learning rate :  0.3 , Sum of errors :  70.8912952691\n",
      "Iteration :  327 , Learning rate :  0.3 , Sum of errors :  70.8912915592\n",
      "Iteration :  328 , Learning rate :  0.3 , Sum of errors :  70.8912878867\n",
      "Iteration :  329 , Learning rate :  0.3 , Sum of errors :  70.8912842515\n",
      "Iteration :  330 , Learning rate :  0.3 , Sum of errors :  70.8912806533\n",
      "Iteration :  331 , Learning rate :  0.3 , Sum of errors :  70.8912770922\n",
      "Iteration :  332 , Learning rate :  0.3 , Sum of errors :  70.8912735678\n",
      "Iteration :  333 , Learning rate :  0.3 , Sum of errors :  70.8912700802\n",
      "Iteration :  334 , Learning rate :  0.3 , Sum of errors :  70.8912666291\n",
      "Iteration :  335 , Learning rate :  0.3 , Sum of errors :  70.8912632144\n",
      "Iteration :  336 , Learning rate :  0.3 , Sum of errors :  70.8912598359\n",
      "Iteration :  337 , Learning rate :  0.3 , Sum of errors :  70.8912564937\n",
      "Iteration :  338 , Learning rate :  0.3 , Sum of errors :  70.8912531874\n",
      "Iteration :  339 , Learning rate :  0.3 , Sum of errors :  70.891249917\n",
      "Iteration :  340 , Learning rate :  0.3 , Sum of errors :  70.8912466824\n",
      "Iteration :  341 , Learning rate :  0.3 , Sum of errors :  70.8912434835\n",
      "Iteration :  342 , Learning rate :  0.3 , Sum of errors :  70.8912403201\n",
      "Iteration :  343 , Learning rate :  0.3 , Sum of errors :  70.8912371921\n",
      "Iteration :  344 , Learning rate :  0.3 , Sum of errors :  70.8912340994\n",
      "Iteration :  345 , Learning rate :  0.3 , Sum of errors :  70.8912310419\n",
      "Iteration :  346 , Learning rate :  0.3 , Sum of errors :  70.8912280195\n",
      "Iteration :  347 , Learning rate :  0.3 , Sum of errors :  70.8912250321\n",
      "Iteration :  348 , Learning rate :  0.3 , Sum of errors :  70.8912220795\n",
      "Iteration :  349 , Learning rate :  0.3 , Sum of errors :  70.8912191617\n",
      "Iteration :  350 , Learning rate :  0.3 , Sum of errors :  70.8912162786\n",
      "Iteration :  351 , Learning rate :  0.3 , Sum of errors :  70.8912134301\n",
      "Iteration :  352 , Learning rate :  0.3 , Sum of errors :  70.891210616\n",
      "Iteration :  353 , Learning rate :  0.3 , Sum of errors :  70.8912078363\n",
      "Iteration :  354 , Learning rate :  0.3 , Sum of errors :  70.8912050909\n",
      "Iteration :  355 , Learning rate :  0.3 , Sum of errors :  70.8912023797\n",
      "Iteration :  356 , Learning rate :  0.3 , Sum of errors :  70.8911997026\n",
      "Iteration :  357 , Learning rate :  0.3 , Sum of errors :  70.8911970596\n",
      "Iteration :  358 , Learning rate :  0.3 , Sum of errors :  70.8911944505\n",
      "Iteration :  359 , Learning rate :  0.3 , Sum of errors :  70.8911918753\n",
      "Iteration :  360 , Learning rate :  0.3 , Sum of errors :  70.8911893339\n",
      "Iteration :  361 , Learning rate :  0.3 , Sum of errors :  70.8911868262\n",
      "Iteration :  362 , Learning rate :  0.3 , Sum of errors :  70.8911843521\n",
      "Iteration :  363 , Learning rate :  0.3 , Sum of errors :  70.8911819116\n",
      "Iteration :  364 , Learning rate :  0.3 , Sum of errors :  70.8911795046\n",
      "Iteration :  365 , Learning rate :  0.3 , Sum of errors :  70.891177131\n",
      "Iteration :  366 , Learning rate :  0.3 , Sum of errors :  70.8911747908\n",
      "Iteration :  367 , Learning rate :  0.3 , Sum of errors :  70.8911724839\n",
      "Iteration :  368 , Learning rate :  0.3 , Sum of errors :  70.8911702103\n",
      "Iteration :  369 , Learning rate :  0.3 , Sum of errors :  70.8911679698\n",
      "Iteration :  370 , Learning rate :  0.3 , Sum of errors :  70.8911657624\n",
      "Iteration :  371 , Learning rate :  0.3 , Sum of errors :  70.8911635882\n",
      "Iteration :  372 , Learning rate :  0.3 , Sum of errors :  70.8911614469\n",
      "Iteration :  373 , Learning rate :  0.3 , Sum of errors :  70.8911593386\n",
      "Iteration :  374 , Learning rate :  0.3 , Sum of errors :  70.8911572632\n",
      "Iteration :  375 , Learning rate :  0.3 , Sum of errors :  70.8911552207\n",
      "Iteration :  376 , Learning rate :  0.3 , Sum of errors :  70.891153211\n",
      "Iteration :  377 , Learning rate :  0.3 , Sum of errors :  70.8911512341\n",
      "Iteration :  378 , Learning rate :  0.3 , Sum of errors :  70.89114929\n",
      "Iteration :  379 , Learning rate :  0.3 , Sum of errors :  70.8911473785\n",
      "Iteration :  380 , Learning rate :  0.3 , Sum of errors :  70.8911454997\n",
      "Iteration :  381 , Learning rate :  0.3 , Sum of errors :  70.8911436536\n",
      "Iteration :  382 , Learning rate :  0.3 , Sum of errors :  70.89114184\n",
      "Iteration :  383 , Learning rate :  0.3 , Sum of errors :  70.891140059\n",
      "Iteration :  384 , Learning rate :  0.3 , Sum of errors :  70.8911383105\n",
      "Iteration :  385 , Learning rate :  0.3 , Sum of errors :  70.8911365946\n",
      "Iteration :  386 , Learning rate :  0.3 , Sum of errors :  70.8911349111\n",
      "Iteration :  387 , Learning rate :  0.3 , Sum of errors :  70.8911332601\n",
      "Iteration :  388 , Learning rate :  0.3 , Sum of errors :  70.8911316415\n",
      "Iteration :  389 , Learning rate :  0.3 , Sum of errors :  70.8911300554\n",
      "Iteration :  390 , Learning rate :  0.3 , Sum of errors :  70.8911285017\n",
      "Iteration :  391 , Learning rate :  0.3 , Sum of errors :  70.8911269803\n",
      "Iteration :  392 , Learning rate :  0.3 , Sum of errors :  70.8911254913\n",
      "Iteration :  393 , Learning rate :  0.3 , Sum of errors :  70.8911240347\n",
      "Iteration :  394 , Learning rate :  0.3 , Sum of errors :  70.8911226105\n",
      "Iteration :  395 , Learning rate :  0.3 , Sum of errors :  70.8911212186\n",
      "Iteration :  396 , Learning rate :  0.3 , Sum of errors :  70.8911198591\n",
      "Iteration :  397 , Learning rate :  0.3 , Sum of errors :  70.8911185319\n",
      "Iteration :  398 , Learning rate :  0.3 , Sum of errors :  70.891117237\n",
      "Iteration :  399 , Learning rate :  0.3 , Sum of errors :  70.8911159745\n",
      "Iteration :  400 , Learning rate :  0.3 , Sum of errors :  70.8911147444\n",
      "Iteration :  401 , Learning rate :  0.3 , Sum of errors :  70.8911135466\n",
      "Iteration :  402 , Learning rate :  0.3 , Sum of errors :  70.8911123811\n",
      "Iteration :  403 , Learning rate :  0.3 , Sum of errors :  70.8911112481\n",
      "Iteration :  404 , Learning rate :  0.3 , Sum of errors :  70.8911101474\n",
      "Iteration :  405 , Learning rate :  0.3 , Sum of errors :  70.8911090791\n",
      "Iteration :  406 , Learning rate :  0.3 , Sum of errors :  70.8911080432\n",
      "Iteration :  407 , Learning rate :  0.3 , Sum of errors :  70.8911070398\n",
      "Iteration :  408 , Learning rate :  0.3 , Sum of errors :  70.8911060688\n",
      "Iteration :  409 , Learning rate :  0.3 , Sum of errors :  70.8911051303\n",
      "Iteration :  410 , Learning rate :  0.3 , Sum of errors :  70.8911042243\n",
      "Iteration :  411 , Learning rate :  0.3 , Sum of errors :  70.8911033508\n",
      "Iteration :  412 , Learning rate :  0.3 , Sum of errors :  70.8911025098\n",
      "Iteration :  413 , Learning rate :  0.3 , Sum of errors :  70.8911017015\n",
      "Iteration :  414 , Learning rate :  0.3 , Sum of errors :  70.8911009257\n",
      "Iteration :  415 , Learning rate :  0.3 , Sum of errors :  70.8911001827\n",
      "Iteration :  416 , Learning rate :  0.3 , Sum of errors :  70.8910994723\n",
      "Iteration :  417 , Learning rate :  0.3 , Sum of errors :  70.8910987946\n",
      "Iteration :  418 , Learning rate :  0.3 , Sum of errors :  70.8910981497\n",
      "Iteration :  419 , Learning rate :  0.3 , Sum of errors :  70.8910975376\n",
      "Iteration :  420 , Learning rate :  0.3 , Sum of errors :  70.8910969584\n",
      "Iteration :  421 , Learning rate :  0.3 , Sum of errors :  70.8910964121\n",
      "Iteration :  422 , Learning rate :  0.3 , Sum of errors :  70.8910958987\n",
      "Iteration :  423 , Learning rate :  0.3 , Sum of errors :  70.8910954184\n",
      "Iteration :  424 , Learning rate :  0.3 , Sum of errors :  70.8910949711\n",
      "Iteration :  425 , Learning rate :  0.3 , Sum of errors :  70.8910945569\n",
      "Iteration :  426 , Learning rate :  0.3 , Sum of errors :  70.8910941759\n",
      "Iteration :  427 , Learning rate :  0.3 , Sum of errors :  70.8910938282\n",
      "Iteration :  428 , Learning rate :  0.3 , Sum of errors :  70.8910935138\n",
      "Iteration :  429 , Learning rate :  0.3 , Sum of errors :  70.8910932327\n",
      "Iteration :  430 , Learning rate :  0.3 , Sum of errors :  70.8910929851\n",
      "Iteration :  431 , Learning rate :  0.3 , Sum of errors :  70.891092771\n",
      "Iteration :  432 , Learning rate :  0.3 , Sum of errors :  70.8910925906\n",
      "Iteration :  433 , Learning rate :  0.3 , Sum of errors :  70.8910924437\n",
      "Iteration :  434 , Learning rate :  0.3 , Sum of errors :  70.8910923306\n",
      "Iteration :  435 , Learning rate :  0.3 , Sum of errors :  70.8910922514\n",
      "Iteration :  436 , Learning rate :  0.3 , Sum of errors :  70.891092206\n",
      "Iteration :  437 , Learning rate :  0.3 , Sum of errors :  70.8910921947\n",
      "Iteration :  438 , Learning rate :  0.3 , Sum of errors :  70.8910922174\n",
      "Iteration :  439 , Learning rate :  0.3 , Sum of errors :  70.8910922743\n",
      "Iteration :  440 , Learning rate :  0.3 , Sum of errors :  70.8910923655\n",
      "Iteration :  441 , Learning rate :  0.3 , Sum of errors :  70.891092491\n",
      "Iteration :  442 , Learning rate :  0.3 , Sum of errors :  70.891092651\n",
      "Iteration :  443 , Learning rate :  0.3 , Sum of errors :  70.8910928456\n",
      "Iteration :  444 , Learning rate :  0.3 , Sum of errors :  70.8910930748\n",
      "Iteration :  445 , Learning rate :  0.3 , Sum of errors :  70.8910933388\n",
      "Iteration :  446 , Learning rate :  0.3 , Sum of errors :  70.8910936377\n",
      "Iteration :  447 , Learning rate :  0.3 , Sum of errors :  70.8910939716\n",
      "Iteration :  448 , Learning rate :  0.3 , Sum of errors :  70.8910943406\n",
      "Iteration :  449 , Learning rate :  0.3 , Sum of errors :  70.8910947448\n",
      "Iteration :  450 , Learning rate :  0.3 , Sum of errors :  70.8910951844\n",
      "Iteration :  451 , Learning rate :  0.3 , Sum of errors :  70.8910956595\n",
      "Iteration :  452 , Learning rate :  0.3 , Sum of errors :  70.8910961701\n",
      "Iteration :  453 , Learning rate :  0.3 , Sum of errors :  70.8910967165\n",
      "Iteration :  454 , Learning rate :  0.3 , Sum of errors :  70.8910972988\n",
      "Iteration :  455 , Learning rate :  0.3 , Sum of errors :  70.891097917\n",
      "Iteration :  456 , Learning rate :  0.3 , Sum of errors :  70.8910985714\n",
      "Iteration :  457 , Learning rate :  0.3 , Sum of errors :  70.8910992621\n",
      "Iteration :  458 , Learning rate :  0.3 , Sum of errors :  70.8910999892\n",
      "Iteration :  459 , Learning rate :  0.3 , Sum of errors :  70.8911007529\n",
      "Iteration :  460 , Learning rate :  0.3 , Sum of errors :  70.8911015532\n",
      "Iteration :  461 , Learning rate :  0.3 , Sum of errors :  70.8911023905\n",
      "Iteration :  462 , Learning rate :  0.3 , Sum of errors :  70.8911032648\n",
      "Iteration :  463 , Learning rate :  0.3 , Sum of errors :  70.8911041762\n",
      "Iteration :  464 , Learning rate :  0.3 , Sum of errors :  70.8911051251\n",
      "Iteration :  465 , Learning rate :  0.3 , Sum of errors :  70.8911061114\n",
      "Iteration :  466 , Learning rate :  0.3 , Sum of errors :  70.8911071354\n",
      "Iteration :  467 , Learning rate :  0.3 , Sum of errors :  70.8911081973\n",
      "Iteration :  468 , Learning rate :  0.3 , Sum of errors :  70.8911092973\n",
      "Iteration :  469 , Learning rate :  0.3 , Sum of errors :  70.8911104354\n",
      "Iteration :  470 , Learning rate :  0.3 , Sum of errors :  70.891111612\n",
      "Iteration :  471 , Learning rate :  0.3 , Sum of errors :  70.8911128272\n",
      "Iteration :  472 , Learning rate :  0.3 , Sum of errors :  70.8911140811\n",
      "Iteration :  473 , Learning rate :  0.3 , Sum of errors :  70.8911153741\n",
      "Iteration :  474 , Learning rate :  0.3 , Sum of errors :  70.8911167062\n",
      "Iteration :  475 , Learning rate :  0.3 , Sum of errors :  70.8911180777\n",
      "Iteration :  476 , Learning rate :  0.3 , Sum of errors :  70.8911194887\n",
      "Iteration :  477 , Learning rate :  0.3 , Sum of errors :  70.8911209396\n",
      "Iteration :  478 , Learning rate :  0.3 , Sum of errors :  70.8911224305\n",
      "Iteration :  479 , Learning rate :  0.3 , Sum of errors :  70.8911239616\n",
      "Iteration :  480 , Learning rate :  0.3 , Sum of errors :  70.8911255332\n",
      "Iteration :  481 , Learning rate :  0.3 , Sum of errors :  70.8911271454\n",
      "Iteration :  482 , Learning rate :  0.3 , Sum of errors :  70.8911287986\n",
      "Iteration :  483 , Learning rate :  0.3 , Sum of errors :  70.8911304929\n",
      "Iteration :  484 , Learning rate :  0.3 , Sum of errors :  70.8911322286\n",
      "Iteration :  485 , Learning rate :  0.3 , Sum of errors :  70.8911340059\n",
      "Iteration :  486 , Learning rate :  0.3 , Sum of errors :  70.8911358251\n",
      "Iteration :  487 , Learning rate :  0.3 , Sum of errors :  70.8911376864\n",
      "Iteration :  488 , Learning rate :  0.3 , Sum of errors :  70.8911395901\n",
      "Iteration :  489 , Learning rate :  0.3 , Sum of errors :  70.8911415364\n",
      "Iteration :  490 , Learning rate :  0.3 , Sum of errors :  70.8911435257\n",
      "Iteration :  491 , Learning rate :  0.3 , Sum of errors :  70.8911455581\n",
      "Iteration :  492 , Learning rate :  0.3 , Sum of errors :  70.891147634\n",
      "Iteration :  493 , Learning rate :  0.3 , Sum of errors :  70.8911497537\n",
      "Iteration :  494 , Learning rate :  0.3 , Sum of errors :  70.8911519173\n",
      "Iteration :  495 , Learning rate :  0.3 , Sum of errors :  70.8911541253\n",
      "Iteration :  496 , Learning rate :  0.3 , Sum of errors :  70.8911563778\n",
      "Iteration :  497 , Learning rate :  0.3 , Sum of errors :  70.8911586753\n",
      "Iteration :  498 , Learning rate :  0.3 , Sum of errors :  70.891161018\n",
      "Iteration :  499 , Learning rate :  0.3 , Sum of errors :  70.8911634063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'delta': 2.3556998487622365e-06,\n",
       "   'output': 0.9995998989460628,\n",
       "   'weights': [0.49470682751665829,\n",
       "    0.056952087244238843,\n",
       "    0.14195672678604093,\n",
       "    0.73559258725402832,\n",
       "    0.99914194248607324,\n",
       "    0.60304830296582668,\n",
       "    0.42422493622194418,\n",
       "    0.18421745998064967]},\n",
       "  {'delta': 1.1268857977635933e-09,\n",
       "   'output': 0.9999997445149991,\n",
       "   'weights': [0.50221597101626769,\n",
       "    0.98201612130863958,\n",
       "    0.77036146601328259,\n",
       "    0.53955003393060963,\n",
       "    0.86024933885944033,\n",
       "    0.23215631864753802,\n",
       "    0.51370974518478008,\n",
       "    0.9524449058575325]},\n",
       "  {'delta': 2.181916256656266e-07,\n",
       "   'output': 0.9999557034847774,\n",
       "   'weights': [0.57225909277267628,\n",
       "    0.44511991683859065,\n",
       "    0.23026081496819506,\n",
       "    0.53277840208459581,\n",
       "    0.94699555629654442,\n",
       "    0.0024675947190513571,\n",
       "    0.77068107815523512,\n",
       "    0.8153256110373067]},\n",
       "  {'delta': 6.2502250413812606e-09,\n",
       "   'output': 0.9999985871180664,\n",
       "   'weights': [0.88594603049936749,\n",
       "    0.73989357246967535,\n",
       "    0.80745328349389145,\n",
       "    0.51800934198780169,\n",
       "    0.56093005081218106,\n",
       "    0.42592883303645485,\n",
       "    0.055536764865643136,\n",
       "    0.8697829926639633]},\n",
       "  {'delta': 3.407453406682167e-07,\n",
       "   'output': 0.9999727725200924,\n",
       "   'weights': [0.57839248982277913,\n",
       "    0.22628919175937387,\n",
       "    0.5835750997716993,\n",
       "    0.51473780268128388,\n",
       "    0.37419307561867116,\n",
       "    0.35477549561953725,\n",
       "    0.56557219899914446,\n",
       "    0.6343874150066428]}],\n",
       " [{'delta': -0.0024954903167398994,\n",
       "   'output': 0.05128739114449668,\n",
       "   'weights': [-0.3013076884547317,\n",
       "    -0.3589692433039325,\n",
       "    -0.7946457441638746,\n",
       "    -0.5876674808324434,\n",
       "    -0.6471174563902867,\n",
       "    -0.23261883750979725]},\n",
       "  {'delta': -0.005847672263675469,\n",
       "   'output': 0.07971313706967627,\n",
       "   'weights': [-0.11846108132116301,\n",
       "    -0.37731059452247767,\n",
       "    -0.36411500591648255,\n",
       "    -0.3587236644543099,\n",
       "    -0.9041168687205953,\n",
       "    -0.33413670558313524]},\n",
       "  {'delta': 0.006495695598642507,\n",
       "   'output': 0.9157795897963789,\n",
       "   'weights': [0.688188334743957,\n",
       "    0.20526886372361228,\n",
       "    0.12907816977598946,\n",
       "    0.13613880683594148,\n",
       "    0.8679639060577623,\n",
       "    0.371695686372564]}]]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network = back_propagation(train,0.3,500,5)\n",
    "my_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output = []\n",
    "for i in range(len(train)):\n",
    "    curr = forward_propagate(my_network,train[i])\n",
    "    output.append(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05106627334147904, 0.07894221970075141, 0.9166843197949434]\n",
      "[0.05106756201504071, 0.07894315395681527, 0.9166803590072108]\n",
      "[0.05106771248034762, 0.07894321886172918, 0.9166803001329544]\n",
      "[0.05106796190995481, 0.0789434617202166, 0.9166796076312308]\n",
      "[0.05106544808728486, 0.07894154451376481, 0.9166865054101627]\n",
      "[0.05106753340864485, 0.07894310300288035, 0.916680737750695]\n",
      "[0.051066536015494574, 0.07894239572257034, 0.9166837930867008]\n",
      "[0.051067478534945594, 0.07894308317978055, 0.9166809957702542]\n",
      "[0.05106431633839258, 0.0789407217623846, 0.9166898496456236]\n",
      "[0.0510647398789298, 0.07894103204089017, 0.9166886334904261]\n",
      "[0.05106575534887577, 0.0789418366440379, 0.9166861763054838]\n",
      "[0.05106850860556654, 0.07894384171034965, 0.9166777620212335]\n",
      "[0.051067919586190655, 0.07894343656002478, 0.9166802246684218]\n",
      "[0.05106834803758221, 0.0789437685679392, 0.9166787473600115]\n",
      "[0.051068776490829804, 0.07894408011097666, 0.9166774883593701]\n",
      "[0.05106699233884533, 0.0789427250452382, 0.9166827420723965]\n",
      "[0.051066989158800605, 0.07894269266728808, 0.9166829782704049]\n",
      "[0.05106604034778302, 0.07894197984386638, 0.9166848236100849]\n",
      "[0.051067757205087994, 0.07894323906689653, 0.9166799527455289]\n",
      "[0.051069805586758193, 0.07894487657592014, 0.9166749591129506]\n",
      "[0.051067523723506106, 0.07894318397208501, 0.9166809780989773]\n",
      "[0.05106766736628185, 0.07894323639183908, 0.9166803757344325]\n",
      "[0.051066102101112315, 0.07894202991850194, 0.9166845017446512]\n",
      "[0.051073217514374256, 0.07894729752128297, 0.9166641102953905]\n",
      "[0.051067077058470786, 0.07894283786323288, 0.9166820107311372]\n",
      "[0.051065684317344616, 0.07894172862921191, 0.9166857327551838]\n",
      "[0.05106985074593056, 0.0789449183472495, 0.9166746745185177]\n",
      "[0.05107099129582181, 0.07894576207292354, 0.9166711401116342]\n",
      "[0.051067538925675345, 0.07894313926068455, 0.9166808571023043]\n",
      "[0.051068444478928825, 0.07894387064700825, 0.9166785380250064]\n",
      "[0.051070791415140704, 0.0789454990516087, 0.9166708872798067]\n",
      "[0.05106561455097571, 0.07894174394281521, 0.9166864291644369]\n",
      "[0.05106671961332122, 0.07894262358992242, 0.9166834830795308]\n",
      "[0.051068250816674154, 0.0789436783962647, 0.9166786958361274]\n",
      "[0.051066182128794255, 0.07894212667906655, 0.9166845141222293]\n",
      "[0.05106507562131408, 0.07894125776950843, 0.9166876637113098]\n",
      "[0.05106487662841236, 0.07894117067482868, 0.9166883635356139]\n",
      "[0.05106397308502031, 0.078940449364334, 0.9166909767884026]\n",
      "[0.05106621794098542, 0.07894214889018611, 0.9166846019217699]\n",
      "[0.0510660274433731, 0.07894203473647106, 0.9166859614015578]\n",
      "[0.05106840003884872, 0.07894372543862073, 0.9166782161509992]\n",
      "[0.05106858844512623, 0.0789438675264125, 0.9166775903189495]\n",
      "[0.051069676244955106, 0.07894465280421072, 0.9166746713527982]\n",
      "[0.051064629829955174, 0.07894098234383458, 0.916689379651924]\n",
      "[0.051065703786023736, 0.07894176120391072, 0.9166861010414576]\n",
      "[0.05106921833103903, 0.07894435294846915, 0.9166756587139555]\n",
      "[0.05106635596839503, 0.07894224652982218, 0.9166839132854857]\n",
      "[0.05106619035867569, 0.07894213260453811, 0.9166846700768699]\n",
      "[0.051066872654929686, 0.07894264150246001, 0.916682667690575]\n",
      "[0.051066683255838935, 0.07894251709497752, 0.9166830615788043]\n",
      "[0.051066545929793844, 0.07894243987540361, 0.9166839382247344]\n",
      "[0.0510648626023384, 0.07894115800316016, 0.9166888895079844]\n",
      "[0.051066634939133465, 0.0789425193147886, 0.9166836374449986]\n",
      "[0.051067099871084015, 0.07894278940327049, 0.9166821101268369]\n",
      "[0.05106742897058545, 0.07894306156523301, 0.9166807204250642]\n",
      "[0.05106671048357377, 0.07894251762511928, 0.9166828822505048]\n",
      "[0.05106708538830682, 0.07894283353800044, 0.9166821230370482]\n",
      "[0.05106717100114358, 0.0789428029205964, 0.9166813918258581]\n",
      "[0.05106601267432021, 0.07894199887268598, 0.9166849991107465]\n",
      "[0.051073950695377617, 0.07894815810452664, 0.9166627089585943]\n",
      "[0.051075046841157094, 0.07894868823814521, 0.9166599424600024]\n",
      "[0.05107521392579762, 0.07894869052655158, 0.91665902920999]\n",
      "[0.05107153534960846, 0.07894604780103052, 0.9166698907584092]\n",
      "[0.05106851407239223, 0.0789439038637819, 0.916678493650553]\n",
      "[0.05107185094717575, 0.07894631193433421, 0.916668093264163]\n",
      "[0.051071026175975975, 0.07894569692062282, 0.9166709531187068]\n",
      "[0.05106788649376223, 0.07894338769629358, 0.9166794562249755]\n",
      "[0.05106798196802786, 0.0789435124722132, 0.9166794684223081]\n",
      "[0.0510678884976592, 0.07894336197528137, 0.9166793669577714]\n",
      "[0.051070482644616416, 0.0789454070633857, 0.916672836028766]\n",
      "[0.051063322971493824, 0.07893999539541312, 0.9166929345916447]\n",
      "[0.05106383106636879, 0.07894039766284743, 0.9166915522399346]\n",
      "[0.05106359192764067, 0.07894020342585994, 0.9166922233114996]\n",
      "[0.05106265595365996, 0.07893945436536762, 0.9166947757861733]\n",
      "[0.051063881120839524, 0.07894040359338308, 0.9166913295910419]\n",
      "[0.05106402641253147, 0.07894054085727785, 0.9166910142742859]\n",
      "[0.05106396673198984, 0.07894047903031769, 0.9166910289782738]\n",
      "[0.051062167144840874, 0.07893909432825658, 0.9166962707642757]\n",
      "[0.05106255011873004, 0.07893939047879055, 0.9166952019457751]\n",
      "[0.05106410553112619, 0.07894055234532912, 0.9166905231282568]\n",
      "[0.051063802760414556, 0.0789403443534785, 0.9166916940005161]\n",
      "[0.051062731139817075, 0.07893953345259645, 0.9166947193448058]\n",
      "[0.051062313851239896, 0.07893920715167392, 0.9166958800139565]\n",
      "[0.05106281800475048, 0.07893958351120316, 0.9166941996617458]\n",
      "[0.05106265852602441, 0.07893947505685023, 0.916694775306226]\n",
      "[0.05106319981857431, 0.0789398625466259, 0.9166931149493751]\n",
      "[0.051063154529706486, 0.07893981866045675, 0.9166931814096417]\n",
      "[0.05106258298469699, 0.07893942827282474, 0.9166950392959083]\n",
      "[0.051061890361382575, 0.07893888231049176, 0.9166971463943443]\n",
      "[0.05106198228338356, 0.07893894715811212, 0.9166968394838377]\n",
      "[0.0510624892145711, 0.07893933502623136, 0.9166952105259879]\n",
      "[0.051062868891581896, 0.0789396181020514, 0.9166941597801748]\n",
      "[0.05106293271990291, 0.07893966688714671, 0.9166939806666758]\n",
      "[0.05106261818298293, 0.07893944157858584, 0.9166950908516033]\n",
      "[0.05106270674262364, 0.07893953609713969, 0.9166947755304257]\n",
      "[0.05106381320368453, 0.07894036863784781, 0.9166915030296807]\n",
      "[0.05106259515379755, 0.07893942569623795, 0.916694988140701]\n",
      "[0.05106298675529316, 0.0789397036785373, 0.9166937122673542]\n",
      "[0.05106332664225058, 0.07893997746029285, 0.9166927744661569]\n",
      "[0.051063162006754384, 0.07893984439492313, 0.9166932238304208]\n",
      "[0.05106434100909875, 0.07894075066462923, 0.9166900342870674]\n",
      "[0.05106368312315507, 0.07894021858792324, 0.9166916579641545]\n",
      "[0.05106259199502509, 0.07893941711747142, 0.9166950380252911]\n",
      "[0.05106279153447597, 0.0789395755548224, 0.9166943978695861]\n",
      "[0.05106281556536854, 0.07893959044222087, 0.916694332054986]\n",
      "[0.05106317875003081, 0.07893985894339085, 0.9166932073531462]\n",
      "[0.05106280374369571, 0.07893955497368767, 0.9166943091794136]\n",
      "[0.05106353943777694, 0.07894014065488324, 0.9166922589185371]\n",
      "[0.051062302002641696, 0.07893919488488445, 0.9166958316549992]\n",
      "[0.051063431198107176, 0.07894004238566477, 0.9166924024570242]\n",
      "[0.05106329100572872, 0.07893994761947772, 0.9166928836581538]\n",
      "[0.051062761413837254, 0.0789395705733571, 0.916694539047701]\n",
      "[0.05106285293595441, 0.07893960640910778, 0.9166941593172332]\n",
      "[0.051062505633867404, 0.07893937830571215, 0.9166954477879484]\n",
      "[0.051062016146633554, 0.07893898361426552, 0.916696739119413]\n",
      "[0.05106284896697042, 0.07893960467788216, 0.9166941700064665]\n",
      "[0.05106276724150258, 0.07893955087854009, 0.9166945608473214]\n",
      "[0.05106271894441585, 0.07893951153915475, 0.9166945937394293]\n",
      "[0.05106281398175807, 0.07893957573637185, 0.9166943630398328]\n",
      "[0.0510623920716606, 0.07893926749992311, 0.9166955632455356]\n",
      "[0.05106214375581162, 0.0789390855671135, 0.916696419348257]\n",
      "[0.05106343006766566, 0.07894005168440012, 0.9166925352943914]\n",
      "[0.05106474322095791, 0.07894109139702075, 0.9166888955950702]\n",
      "[0.0510631690002607, 0.0789398324695188, 0.916693254629176]\n",
      "[0.05106538492734919, 0.07894150944488186, 0.9166869428812683]\n",
      "[0.05106271454020333, 0.0789395126000008, 0.9166946920148986]\n",
      "[0.051062970810961575, 0.07893971803570386, 0.9166939530367753]\n",
      "[0.05106351620632408, 0.07894009346538862, 0.9166921735498982]\n",
      "[0.05106265608713234, 0.07893946738102377, 0.9166947215882758]\n",
      "[0.051063414506404926, 0.07894004719422072, 0.9166927827204452]\n",
      "[0.0510631911498314, 0.07893984161622923, 0.91669316883513]\n",
      "[0.05106292227023349, 0.0789396666218722, 0.9166939985446115]\n",
      "[0.05106485430478629, 0.07894113858616761, 0.9166886225692458]\n",
      "[0.051064492632723606, 0.0789408880250565, 0.9166896058321867]\n",
      "[0.05106438285150466, 0.07894078061079914, 0.9166900070460048]\n",
      "[0.0510653644623256, 0.07894148919047292, 0.9166870201402539]\n",
      "[0.051063484236487176, 0.07894010415524604, 0.9166924266255975]\n",
      "[0.05106537455764923, 0.07894153987599478, 0.9166868271831617]\n",
      "[0.05106543096859407, 0.07894158565332425, 0.916686693339399]\n",
      "[0.05106415282525531, 0.07894059318324671, 0.9166904922257358]\n",
      "[0.051067913019135436, 0.0789435091740523, 0.9166804251811846]\n",
      "[0.05106646176080001, 0.07894240703930899, 0.9166848410395508]\n",
      "[0.05106736069287213, 0.0789430712122499, 0.9166821068018823]\n",
      "[0.05106924508106927, 0.07894445476006993, 0.9166768729388874]\n",
      "[0.051071625631256556, 0.07894635430305799, 0.9166701120172144]\n",
      "[0.051071699196375164, 0.07894648041224965, 0.9166707509356685]\n",
      "[0.05107475596390838, 0.07894856259290722, 0.9166601767576947]\n",
      "[0.051069962316442036, 0.0789449808138273, 0.9166746212039396]\n",
      "[0.051069951582377295, 0.07894497414247023, 0.9166740634078081]\n",
      "[0.051072981878448574, 0.07894748984142193, 0.9166671934078033]\n",
      "[0.05107026957173758, 0.07894524283381002, 0.9166739788516297]\n",
      "[0.05106955425803559, 0.0789448421343929, 0.9166767524596413]\n",
      "[0.051069909862096245, 0.07894503243038073, 0.9166748109236529]\n",
      "[0.05107464758555546, 0.07894859188123529, 0.9166613738421322]\n",
      "[0.051072578830919466, 0.07894697682278765, 0.9166671569205422]\n",
      "[0.05107214942954573, 0.0789467572967924, 0.9166693529589578]\n",
      "[0.05107327316886522, 0.07894741393503567, 0.9166649895428751]\n",
      "[0.05107131143281398, 0.07894621245252433, 0.916671128349902]\n",
      "[0.051071897798147595, 0.07894662027472572, 0.9166693338021581]\n",
      "[0.051071562632546905, 0.07894628987423796, 0.9166706220844092]\n",
      "[0.05106982556862515, 0.07894488810007785, 0.9166743296723014]\n",
      "[0.05107054098753107, 0.0789454073725085, 0.9166728492822619]\n",
      "[0.051070948399081044, 0.07894581249610261, 0.9166722095961913]\n",
      "[0.051069465492353215, 0.07894464379101976, 0.9166759468063213]\n",
      "[0.05107185908764573, 0.0789464345549414, 0.916670355319347]\n",
      "[0.0510720414116682, 0.07894637481588411, 0.9166677144408977]\n",
      "[0.05106964963580395, 0.07894480040356255, 0.9166755546334523]\n",
      "[0.051070436569196645, 0.07894535612918574, 0.9166729294331163]\n",
      "[0.05107364631812288, 0.07894780624209449, 0.916664430187908]\n",
      "[0.05107441350698327, 0.0789483379580894, 0.9166618630264277]\n",
      "[0.05107147059833438, 0.07894634055518344, 0.9166718396936286]\n",
      "[0.051070826715375404, 0.07894575773537912, 0.9166734284525352]\n",
      "[0.05107351579132962, 0.07894769306829319, 0.9166648784346804]\n",
      "[0.05107210816365871, 0.07894668904511437, 0.9166693219471376]\n",
      "[0.0510734324361599, 0.0789478202477385, 0.9166657797519828]\n",
      "[0.05107305208026873, 0.07894728889222428, 0.9166663728055883]\n",
      "[0.0510726035788533, 0.07894704848523744, 0.9166679058649047]\n",
      "[0.05107477158488903, 0.07894866892228883, 0.9166618912324633]\n",
      "[0.05107177595127474, 0.07894639678896787, 0.9166704127014093]\n",
      "[0.051071898922176014, 0.07894642824374931, 0.9166680783626263]\n",
      "[0.05107306291419373, 0.07894730520391527, 0.9166665396933377]\n",
      "[0.05106912004889477, 0.07894434810344043, 0.9166770641601261]\n",
      "[0.05107011163477019, 0.07894510725893615, 0.9166743038611083]\n",
      "[0.051071187064794715, 0.07894590285568913, 0.9166715060813692]\n",
      "[0.051067498303888716, 0.07894320964937131, 0.9166819074385988]\n",
      "[0.051072719746436894, 0.07894717884098433, 0.9166669679098427]\n",
      "[0.05107107247005813, 0.07894594877311939, 0.9166717513489412]\n",
      "[0.051074967068979224, 0.07894877270540906, 0.9166609254466942]\n",
      "[0.05107079998234735, 0.07894567304122226, 0.9166738407196586]\n",
      "[0.05107451403275392, 0.07894835598105952, 0.9166627211917066]\n",
      "[0.051073489474745576, 0.07894770221670953, 0.916665550969591]\n",
      "[0.05107311209496496, 0.07894731518138971, 0.9166657979903051]\n",
      "[0.05107089139414494, 0.07894557559536618, 0.916671600413506]\n",
      "[0.05107423910738805, 0.07894830171959855, 0.916663323587657]\n",
      "[0.051070425029307254, 0.07894534346402082, 0.9166732969608934]\n",
      "[0.051068909376116085, 0.07894416976842077, 0.9166776582012738]\n",
      "[0.0510688154138579, 0.07894411484263206, 0.9166781162217664]\n",
      "[0.05106793457154939, 0.07894342559974779, 0.9166801868725]\n",
      "[0.05107009561174342, 0.07894510157627067, 0.9166737661282369]\n",
      "[0.051070163949148574, 0.07894502382136105, 0.9166734032638666]\n",
      "[0.051069478563910746, 0.07894466897512731, 0.916676340931838]\n",
      "[0.051071189911984144, 0.07894575253784805, 0.9166702724548358]\n",
      "[0.05107378005587742, 0.078947757539868, 0.9166640593695272]\n",
      "[0.05106737065235653, 0.07894305446602551, 0.916682929885578]\n",
      "[0.05107073815662758, 0.07894559035801262, 0.9166722733900304]\n",
      "[0.051071123787171754, 0.07894578235082379, 0.9166711102524223]\n",
      "[0.05107283913993061, 0.07894715616241312, 0.9166667972079445]\n",
      "[0.051066443166679756, 0.07894237989528298, 0.9166852921898184]\n",
      "[0.05107227101079013, 0.07894670079229049, 0.9166678575785601]\n",
      "[0.05106930358156102, 0.07894450310139318, 0.9166768695927111]\n"
     ]
    }
   ],
   "source": [
    "for x in output:\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
